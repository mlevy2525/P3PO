{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 243.9041095890411, 203.01492267341956), (0, 282.73972602739724, 179.1791625214796), (0, 247.8082191780822, 235.06991046395947), (0, 290.958904109589, 211.64511169394953), (0, 266.71232876712327, 211.85059238491453), (0, 250.27397260273972, 276.1660486569594), (0, 289.52054794520546, 248.83711675861446)]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 24\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# print(len(data['episode_list'][0]))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# print(data[\"observations\"][0][0])\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m dem \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mobservations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m][pixel_key]\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(dem\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_cartesian\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_pickle = True\n",
    "# pickle_path = \"/home/aadhithya/bobby_wks/P3PO/processed_data/points/microwave.pkl\"\n",
    "# pickle_path =\"/home/aadhithya/bobby_wks/P3PO/processed_data/points/1218_pick_bottle_from_fridge.pkl\"\n",
    "# pickle_path = \"/home/aadhithya/bobby_wks/P3PO/processed_data/points/1220_pick_up_bottle.pkl\"\n",
    "pickle_path = \"/home/aadhithya/bobby_wks/P3PO/coordinates/coords/cam4_robot_place_bottle_8p.pkl\"\n",
    "pixel_key = \"pixels4\"\n",
    "\n",
    "#TODO: Set the task name here -- this will be used to save the output\n",
    "task_name = \"assembly\"\n",
    "\n",
    "if use_pickle:\n",
    "    import pickle\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(data)\n",
    "    # print(len(data['episode_list'][0]))\n",
    "\n",
    "    # print(data[\"observations\"][0][0])\n",
    "    dem = data['observations'][0][pixel_key]\n",
    "    print(dem.shape)\n",
    "    print(data['max_cartesian'])\n",
    "    print(data['min_cartesian'])\n",
    "    print(data['max_gripper'])\n",
    "    print(data['min_gripper'])\n",
    "    print(data['max_sensor'])\n",
    "    print(data['min_sensor'])\n",
    "    # print(img1.shape)\n",
    "    # img2 = data['observations'][0][pixel_key][1]\n",
    "    # img3 = data['observations'][0][pixel_key][2]\n",
    "    # img4 = data['observations'][0][pixel_key][3]\n",
    "    # img5 = data['observations'][0][pixel_key][60]\n",
    "    # plt.imshow(img1)\n",
    "    # plt.show()\n",
    "    # plt.imshow(img2)\n",
    "    # plt.show()\n",
    "    # plt.imshow(img3)\n",
    "    # plt.show()\n",
    "    # plt.imshow(img4)\n",
    "    # plt.show()\n",
    "    # plt.imshow(img5)\n",
    "    # plt.show()\n",
    "\n",
    "    # use_video = False\n",
    "    # check data keys\n",
    "    # print(data.keys())\n",
    "    # print(data['episode_list'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['observations', 'max_cartesian', 'min_cartesian', 'max_gripper', 'min_gripper', 'max_sensor', 'min_sensor'])\n",
      "(247, 360, 360, 3)\n",
      "[3.3824637e+02 1.9113258e+02 4.8120712e+02 3.1409099e+00 4.1035000e-02\n",
      " 4.8312999e-02]\n",
      "[202.65956  -80.48707  358.87817   -3.141357  -1.544027  -0.891604]\n",
      "1.0\n",
      "0.0\n",
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_pickle = True\n",
    "# pickle_path = \"/home/aadhithya/bobby_wks/P3PO/processed_data/points/microwave.pkl\"\n",
    "# pickle_path =\"/home/aadhithya/bobby_wks/P3PO/processed_data/points/1218_pick_bottle_from_fridge.pkl\"\n",
    "# pickle_path = \"/home/aadhithya/bobby_wks/P3PO/processed_data/points/1220_pick_up_bottle.pkl\"\n",
    "pickle_path = \"/home/aadhithya/bobby_wks/P3PO/expert_demos/xarm_env/1220_pick_bottle_from_fridge_compare.pkl\"\n",
    "pixel_key = \"pixels4\"\n",
    "\n",
    "#TODO: Set the task name here -- this will be used to save the output\n",
    "task_name = \"assembly\"\n",
    "\n",
    "if use_pickle:\n",
    "    import pickle\n",
    "    with open(pickle_path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "\n",
    "    print(data.keys()) # 'max_cartesian', 'min_cartesian', 'max_gripper', 'min_gripper', 'max_sensor', 'min_sensor']\n",
    "    # print(len(data['episode_list'][0]))\n",
    "\n",
    "    # print(data[\"observations\"][0][0])\n",
    "    dem = data['observations'][0][pixel_key]\n",
    "    print(dem.shape)\n",
    "    print(data['max_cartesian'])\n",
    "    print(data['min_cartesian'])\n",
    "    print(data['max_gripper'])\n",
    "    print(data['min_gripper'])\n",
    "    print(data['max_sensor'])\n",
    "    print(data['min_sensor'])\n",
    "    \n",
    "    # print(img1.shape)\n",
    "    # img2 = data['observations'][0][pixel_key][1]\n",
    "    # img3 = data['observations'][0][pixel_key][2]\n",
    "    # img4 = data['observations'][0][pixel_key][3]\n",
    "    # img5 = data['observations'][0][pixel_key][60]\n",
    "    # plt.imshow(img1)\n",
    "    # plt.show()\n",
    "    # plt.imshow(img2)\n",
    "    # plt.show()\n",
    "    # plt.imshow(img3)\n",
    "    # plt.show()\n",
    "    # plt.imshow(img4)\n",
    "    # plt.show()\n",
    "    # plt.imshow(img5)\n",
    "    # plt.show()\n",
    "\n",
    "    # use_video = False\n",
    "    # check data keys\n",
    "    # print(data.keys())\n",
    "    # print(data['episode_list'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "video_path = \"/home/aadhithya/bobby_wks/plate_test/demonstration_2/videos/camera2.mp4\"\n",
    "video_dscp = \"multiple_cups\"\n",
    "\n",
    "# get the first frame and save it as png\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "ret, frame = cap.read()\n",
    "cv2.imwrite(f'/home/aadhithya/bobby_wks/{video_dscp}.png', frame)\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pickle\n",
    "import cv2\n",
    "import yaml\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "\n",
    "from points_class import PointsClass\n",
    "\n",
    "# TODO: Set if you want to read from a pickle or from mp4 files\n",
    "# If you are reading from a pickle please make sure that the images are RGB not BGR\n",
    "read_from_pickle = True\n",
    "pickle_path = \"/home/aadhithya/bobby_wks/P3PO/expert_demos/xarm_env/pick_plate_from_rack_color.pkl\"\n",
    "pickle_image_key = \"pixels2\"\n",
    "\n",
    "# TODO: If you want to use gt depth, set to True and set the key for the depth in the pickle\n",
    "# To use gt depth, the depth must be in the same pickle as the images\n",
    "# We assume the input depth is in the form width x height\n",
    "use_gt_depth = False\n",
    "gt_depth_key = \"depth\"\n",
    "\n",
    "# Otherwise we need to add videos to a list\n",
    "# TODO: A list of videos to read from if you are not loading data from a pickle\n",
    "# video_paths = [\"/home/aadhithya/bobby_wks/vstar/0723_open_microwave/demonstration_0/videos/camera2.mp4\"]\n",
    "video_paths = [\"/home/aadhithya/bobby_wks/pick_white_plate.mp4\"]\n",
    "\n",
    "# TODO: Set to true if you want to save a video of the points being tracked\n",
    "write_videos = True\n",
    "\n",
    "# TODO:  If you want to subsample the frames, set the subsample rate here.\n",
    "subsample = 1\n",
    "\n",
    "with open(\"../cfgs/suite/p3po.yaml\") as stream:\n",
    "    try:\n",
    "        cfg = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "if read_from_pickle:\n",
    "    examples = pickle.load(open(pickle_path, \"rb\"))\n",
    "    num_demos = len(examples['observations'])\n",
    "else:\n",
    "    num_demos = len(video_paths)\n",
    "\n",
    "# Initialize the PointsClass object\n",
    "points_class = PointsClass(**cfg)\n",
    "episode_list = []\n",
    "\n",
    "mark_every = 8\n",
    "for i in range(num_demos):\n",
    "    # Read the frames from the pickle or video, these frames must be in RGB so if reading from a pickle make sure to convert if necessary\n",
    "    if read_from_pickle:\n",
    "        frames = examples['observations'][i][pickle_image_key][0::subsample]\n",
    "        if use_gt_depth:\n",
    "            depth = examples['observations'][i][gt_depth_key][0::subsample]\n",
    "    else:\n",
    "        frames = []\n",
    "        video = cv2.VideoCapture(video_paths[i])\n",
    "        subsample_counter = 0\n",
    "        while video.isOpened():\n",
    "            ret, frame = video.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            if subsample_counter % subsample == 0:\n",
    "                # CV2 reads in BGR format, so we need to convert to RGB\n",
    "                frames.append(frame[:, :, ::-1])\n",
    "            subsample_counter += 1\n",
    "        video.release()\n",
    "\n",
    "    points_class.add_to_image_list(frames[0])\n",
    "    points_class.find_semantic_similar_points()\n",
    "    points_class.track_points(is_first_step=True)\n",
    "    points_class.track_points(one_frame=(mark_every == 1))\n",
    "\n",
    "    if use_gt_depth:\n",
    "        points_class.set_depth(depth[0])\n",
    "    else:\n",
    "        points_class.get_depth()\n",
    "\n",
    "    points_list = []\n",
    "    points = points_class.get_points()\n",
    "    points_list.append(points[0])\n",
    "\n",
    "    if write_videos:\n",
    "        video_list = []\n",
    "        image = points_class.plot_image()\n",
    "        video_list.append(image[0])\n",
    "\n",
    "    for idx,image in enumerate(frames[1:]):\n",
    "        points_class.add_to_image_list(image)\n",
    "        if use_gt_depth:\n",
    "            points_class.set_depth(depth[idx + 1])\n",
    "\n",
    "        if (idx + 1) % mark_every == 0 or idx == (len(frames) - 2):\n",
    "            to_add = mark_every - (idx + 1) % mark_every\n",
    "            if to_add < mark_every:\n",
    "                for j in range(to_add):\n",
    "                    points_class.add_to_image_list(image)\n",
    "            else:\n",
    "                to_add = 0\n",
    "\n",
    "            points_class.track_points(one_frame=(mark_every == 1))\n",
    "            if not use_gt_depth:\n",
    "                points_class.get_depth(last_n_frames=mark_every)\n",
    "\n",
    "            points = points_class.get_points(last_n_frames=mark_every)\n",
    "            for j in range(mark_every - to_add):\n",
    "                points_list.append(points[j])\n",
    "\n",
    "            if write_videos:\n",
    "                images = points_class.plot_image(last_n_frames=mark_every)\n",
    "                for j in range(mark_every - to_add):\n",
    "                    video_list.append(images[j])\n",
    "\n",
    "    if write_videos:\n",
    "        imageio.mimsave(f\"videos/{cfg['task_name']}_%d.mp4\" % i, video_list, fps=30)\n",
    "    \n",
    "    episode_list.append(torch.stack(points_list))\n",
    "    points_class.reset_episode()\n",
    "\n",
    "final_graph = {}\n",
    "final_graph['episode_list'] = episode_list\n",
    "final_graph['subsample'] = subsample\n",
    "final_graph['pixel_key'] = pickle_image_key\n",
    "final_graph['use_gt_depth'] = use_gt_depth\n",
    "final_graph['gt_depth_key'] = gt_depth_key\n",
    "final_graph['pickle_path'] = pickle_path\n",
    "final_graph['video_paths'] = video_paths\n",
    "final_graph['cfg'] = cfg\n",
    "\n",
    "pickle.dump(final_graph, open(f\"{cfg['root_dir']}/processed_data/points/{cfg['task_name']}.pkl\", \"wb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 44\u001b[0m\n\u001b[1;32m     42\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mimread(image_paths[i])\n\u001b[1;32m     43\u001b[0m points_class\u001b[38;5;241m.\u001b[39madd_to_image_list(image)\n\u001b[0;32m---> 44\u001b[0m \u001b[43mpoints_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_semantic_similar_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m points_class\u001b[38;5;241m.\u001b[39mtrack_points(is_first_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     46\u001b[0m points_class\u001b[38;5;241m.\u001b[39mtrack_points(one_frame\u001b[38;5;241m=\u001b[39m(mark_every \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/bobby_wks/P3PO/p3po/data_generation/../points_class.py:112\u001b[0m, in \u001b[0;36mPointsClass.find_semantic_similar_points\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_semantic_similar_points\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    Find the semantic similar points between the expert image and the current image.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msemantic_similar_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcorrespondence_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_correspondence\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpert_correspondence_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage_list\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitial_coords\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/bobby_wks/P3PO/p3po/data_generation/../utilities/correspondence.py:94\u001b[0m, in \u001b[0;36mCorrespondence.find_correspondence\u001b[0;34m(self, expert_img_features, current_image, coords)\u001b[0m\n\u001b[1;32m     91\u001b[0m current_image \u001b[38;5;241m=\u001b[39m transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mheight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwidth))(current_image)\n\u001b[1;32m     92\u001b[0m current_image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdift\u001b[38;5;241m.\u001b[39mforward(((current_image \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m.5\u001b[39m)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m), prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompt, ensemble_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble_size, up_ft_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdift_layer, t\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdift_steps)\n\u001b[0;32m---> 94\u001b[0m ft \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mexpert_img_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_image_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m num_channel \u001b[38;5;241m=\u001b[39m ft\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     96\u001b[0m src_ft \u001b[38;5;241m=\u001b[39m ft[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:3 and cuda:0! (when checking argument for argument tensors in method wrapper_CUDA_cat)"
     ]
    }
   ],
   "source": [
    "# set visible cuda devices to be 3\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "# image version of the points\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pickle\n",
    "import cv2\n",
    "import yaml\n",
    "import imageio\n",
    "\n",
    "import torch\n",
    "\n",
    "from points_class import PointsClass\n",
    "\n",
    "# TODO: If you want to use gt depth, set to True and set the key for the depth in the pickle\n",
    "# To use gt depth, the depth must be in the same pickle as the images\n",
    "# We assume the input depth is in the form width x height\n",
    "use_gt_depth = False\n",
    "gt_depth_key = \"depth\"\n",
    "\n",
    "# add images to a list\n",
    "image_paths = [\"/home/aadhithya/bobby_wks/multiagent/black_plate.png\", \"/home/aadhithya/bobby_wks/multiagent/white_plate.png\", \"/home/aadhithya/bobby_wks/multiagent/blue_plate.png\"]\n",
    "\n",
    "write_images = True\n",
    "\n",
    "with open(\"../cfgs/suite/p3po.yaml\") as stream:\n",
    "    try:\n",
    "        cfg = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)\n",
    "\n",
    "# Initialize the PointsClass object\n",
    "points_class = PointsClass(**cfg)\n",
    "episode_list = []\n",
    "\n",
    "mark_every = 8\n",
    "\n",
    "for i in range(len(image_paths)):\n",
    "    image = cv2.imread(image_paths[i])\n",
    "    points_class.add_to_image_list(image)\n",
    "    points_class.find_semantic_similar_points()\n",
    "    points_class.track_points(is_first_step=True)\n",
    "    points_class.track_points(one_frame=(mark_every == 1))\n",
    "\n",
    "    if use_gt_depth:\n",
    "        points_class.set_depth(depth[0])\n",
    "    else:\n",
    "        points_class.get_depth()\n",
    "\n",
    "    points_list = []\n",
    "    points = points_class.get_points()\n",
    "    points_list.append(points[0])\n",
    "\n",
    "    if write_images:\n",
    "        image = points_class.plot_image()\n",
    "        cv2.imwrite(f\"{image_paths[i]}\", image[0])\n",
    "\n",
    "    episode_list.append(torch.stack(points_list))\n",
    "    points_class.reset_episode()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p3po",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
